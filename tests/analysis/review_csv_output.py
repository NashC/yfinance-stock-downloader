#!/usr/bin/env python3
"""
CSV Output Review Script

This script analyzes and reviews the CSV files generated by the stock price downloader.
It provides comprehensive statistics, data quality checks, and visualizations.
"""

import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import argparse
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

def analyze_csv_file(csv_path: Path) -> Dict:
    """Analyze a single CSV file and return comprehensive statistics."""
    try:
        # Read the CSV file with improved parsing
        df = pd.read_csv(csv_path, index_col=0, parse_dates=True)
        
        # Handle any remaining multi-level header issues from old files
        if len(df.columns) > 0 and df.iloc[0].dtype == 'object':
            # Check if first row contains ticker symbols (old format)
            first_row_values = df.iloc[0].values
            if any(isinstance(val, str) and val.isupper() and len(val) <= 5 for val in first_row_values):
                # Skip the ticker row and reset column names
                df = df.iloc[1:].copy()
                # Ensure we have standard column names
                expected_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']
                if len(df.columns) == len(expected_cols):
                    df.columns = expected_cols
        
        # Convert all columns to numeric, handling any string values
        for col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # Drop any rows that are completely NaN (from parsing issues)
        df = df.dropna(how='all')
        
        # Basic info
        ticker = csv_path.stem
        total_rows = len(df)
        
        if total_rows == 0:
            return {
                'ticker': ticker,
                'file_path': str(csv_path),
                'error': 'No valid data rows found',
                'data_quality_score': 0
            }
        
        # Date range analysis
        start_date = df.index.min()
        end_date = df.index.max()
        date_range_days = (end_date - start_date).days
        
        # Expected columns for stock data
        expected_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']
        missing_columns = [col for col in expected_columns if col not in df.columns]
        extra_columns = [col for col in df.columns if col not in expected_columns]
        
        # Data quality checks
        null_counts = df.isnull().sum()
        zero_volume_days = (df['Volume'] == 0).sum() if 'Volume' in df.columns else 0
        
        # Price analysis
        price_stats = {}
        if 'Close' in df.columns:
            close_prices = df['Close'].dropna()
            if len(close_prices) > 0:
                price_stats = {
                    'min_price': close_prices.min(),
                    'max_price': close_prices.max(),
                    'mean_price': close_prices.mean(),
                    'median_price': close_prices.median(),
                    'std_price': close_prices.std(),
                    'price_range': close_prices.max() - close_prices.min()
                }
                
                # Calculate returns
                daily_returns = close_prices.pct_change().dropna()
                if len(daily_returns) > 0:
                    price_stats.update({
                        'mean_daily_return': daily_returns.mean(),
                        'std_daily_return': daily_returns.std(),
                        'min_daily_return': daily_returns.min(),
                        'max_daily_return': daily_returns.max()
                    })
        
        # Volume analysis
        volume_stats = {}
        if 'Volume' in df.columns:
            volumes = df['Volume'].dropna()
            if len(volumes) > 0:
                volume_stats = {
                    'min_volume': volumes.min(),
                    'max_volume': volumes.max(),
                    'mean_volume': volumes.mean(),
                    'median_volume': volumes.median(),
                    'zero_volume_days': zero_volume_days
                }
        
        # Data consistency checks
        consistency_issues = []
        if all(col in df.columns for col in ['Open', 'High', 'Low', 'Close']):
            # Check if High >= Low, High >= Open, High >= Close, Low <= Open, Low <= Close
            valid_data = df[['Open', 'High', 'Low', 'Close']].dropna()
            if len(valid_data) > 0:
                high_low_issues = (valid_data['High'] < valid_data['Low']).sum()
                high_open_issues = (valid_data['High'] < valid_data['Open']).sum()
                high_close_issues = (valid_data['High'] < valid_data['Close']).sum()
                low_open_issues = (valid_data['Low'] > valid_data['Open']).sum()
                low_close_issues = (valid_data['Low'] > valid_data['Close']).sum()
                
                if high_low_issues > 0:
                    consistency_issues.append(f"High < Low: {high_low_issues} days")
                if high_open_issues > 0:
                    consistency_issues.append(f"High < Open: {high_open_issues} days")
                if high_close_issues > 0:
                    consistency_issues.append(f"High < Close: {high_close_issues} days")
                if low_open_issues > 0:
                    consistency_issues.append(f"Low > Open: {low_open_issues} days")
                if low_close_issues > 0:
                    consistency_issues.append(f"Low > Close: {low_close_issues} days")
        
        # Recent data check (last 30 days)
        recent_cutoff = datetime.now() - timedelta(days=30)
        has_recent_data = end_date >= recent_cutoff
        days_since_last_update = (datetime.now() - end_date).days
        
        return {
            'ticker': ticker,
            'file_path': str(csv_path),
            'file_size_mb': csv_path.stat().st_size / (1024 * 1024),
            'total_rows': total_rows,
            'columns': list(df.columns),
            'missing_columns': missing_columns,
            'extra_columns': extra_columns,
            'start_date': start_date,
            'end_date': end_date,
            'date_range_days': date_range_days,
            'has_recent_data': has_recent_data,
            'days_since_last_update': days_since_last_update,
            'null_counts': null_counts.to_dict(),
            'total_nulls': null_counts.sum(),
            'zero_volume_days': zero_volume_days,
            'price_stats': price_stats,
            'volume_stats': volume_stats,
            'consistency_issues': consistency_issues,
            'data_quality_score': calculate_quality_score(df, consistency_issues, null_counts)
        }
        
    except Exception as e:
        return {
            'ticker': csv_path.stem,
            'file_path': str(csv_path),
            'error': str(e),
            'data_quality_score': 0
        }

def calculate_quality_score(df: pd.DataFrame, consistency_issues: List[str], null_counts: pd.Series) -> float:
    """Calculate a data quality score from 0-100."""
    score = 100.0
    
    # Deduct points for missing data
    null_percentage = (null_counts.sum() / (len(df) * len(df.columns))) * 100
    score -= null_percentage * 2  # 2 points per % of missing data
    
    # Deduct points for consistency issues
    score -= len(consistency_issues) * 5  # 5 points per consistency issue type
    
    # Deduct points if data is too sparse (less than 100 rows)
    if len(df) < 100:
        score -= (100 - len(df)) * 0.5
    
    return max(0, min(100, score))

def review_directory(directory: str, pattern: str = "*.csv") -> List[Dict]:
    """Review all CSV files in a directory."""
    dir_path = Path(directory)
    
    if not dir_path.exists():
        print(f"❌ Directory not found: {directory}")
        return []
    
    csv_files = list(dir_path.glob(pattern))
    
    if not csv_files:
        print(f"❌ No CSV files found in: {directory}")
        return []
    
    print(f"📁 Found {len(csv_files)} CSV files in {directory}")
    print("🔍 Analyzing files...")
    
    results = []
    for csv_file in csv_files:
        print(f"  → Analyzing {csv_file.name}...")
        result = analyze_csv_file(csv_file)
        results.append(result)
    
    return results

def print_summary_report(results: List[Dict]):
    """Print a comprehensive summary report."""
    if not results:
        print("❌ No data to analyze")
        return
    
    # Filter out error results
    valid_results = [r for r in results if 'error' not in r]
    error_results = [r for r in results if 'error' in r]
    
    print("\n" + "=" * 80)
    print("📊 CSV OUTPUT ANALYSIS REPORT")
    print("=" * 80)
    
    # Overall statistics
    print(f"\n📈 OVERALL STATISTICS")
    print(f"  • Total files analyzed: {len(results)}")
    print(f"  • Successfully analyzed: {len(valid_results)}")
    print(f"  • Files with errors: {len(error_results)}")
    
    if error_results:
        print(f"\n❌ FILES WITH ERRORS:")
        for result in error_results:
            print(f"  • {result['ticker']}: {result['error']}")
    
    if not valid_results:
        return
    
    # Data quality overview
    quality_scores = [r['data_quality_score'] for r in valid_results]
    print(f"\n🎯 DATA QUALITY OVERVIEW")
    print(f"  • Average quality score: {np.mean(quality_scores):.1f}/100")
    print(f"  • Median quality score: {np.median(quality_scores):.1f}/100")
    print(f"  • Best quality score: {np.max(quality_scores):.1f}/100")
    print(f"  • Worst quality score: {np.min(quality_scores):.1f}/100")
    
    # High quality files (score >= 90)
    high_quality = [r for r in valid_results if r['data_quality_score'] >= 90]
    print(f"  • High quality files (≥90): {len(high_quality)} ({len(high_quality)/len(valid_results)*100:.1f}%)")
    
    # Low quality files (score < 70)
    low_quality = [r for r in valid_results if r['data_quality_score'] < 70]
    if low_quality:
        print(f"  • Low quality files (<70): {len(low_quality)}")
        print("    Files needing attention:")
        for r in sorted(low_quality, key=lambda x: x['data_quality_score'])[:5]:
            print(f"      - {r['ticker']}: {r['data_quality_score']:.1f}/100")
    
    # Date range analysis
    start_dates = [r['start_date'] for r in valid_results]
    end_dates = [r['end_date'] for r in valid_results]
    
    print(f"\n📅 DATE RANGE ANALYSIS")
    print(f"  • Earliest start date: {min(start_dates).strftime('%Y-%m-%d')}")
    print(f"  • Latest start date: {max(start_dates).strftime('%Y-%m-%d')}")
    print(f"  • Earliest end date: {min(end_dates).strftime('%Y-%m-%d')}")
    print(f"  • Latest end date: {max(end_dates).strftime('%Y-%m-%d')}")
    
    # Recent data check
    recent_data_count = sum(1 for r in valid_results if r['has_recent_data'])
    print(f"  • Files with recent data (last 30 days): {recent_data_count}/{len(valid_results)}")
    
    # Data volume analysis
    total_rows = sum(r['total_rows'] for r in valid_results)
    total_size_mb = sum(r['file_size_mb'] for r in valid_results)
    
    print(f"\n💾 DATA VOLUME")
    print(f"  • Total data points: {total_rows:,}")
    print(f"  • Total file size: {total_size_mb:.1f} MB")
    print(f"  • Average rows per file: {total_rows/len(valid_results):.0f}")
    print(f"  • Average file size: {total_size_mb/len(valid_results):.2f} MB")
    
    # Column consistency
    all_columns = set()
    for r in valid_results:
        all_columns.update(r['columns'])
    
    print(f"\n📋 COLUMN ANALYSIS")
    print(f"  • Unique columns found: {sorted(all_columns)}")
    
    # Missing columns analysis
    missing_cols_count = {}
    for r in valid_results:
        for col in r['missing_columns']:
            missing_cols_count[col] = missing_cols_count.get(col, 0) + 1
    
    if missing_cols_count:
        print(f"  • Missing columns:")
        for col, count in missing_cols_count.items():
            print(f"    - {col}: missing in {count} files")
    
    # Data consistency issues
    consistency_issues_count = sum(len(r['consistency_issues']) for r in valid_results)
    if consistency_issues_count > 0:
        print(f"\n⚠️  DATA CONSISTENCY ISSUES")
        print(f"  • Total files with issues: {sum(1 for r in valid_results if r['consistency_issues'])}")
        print(f"  • Total issue types: {consistency_issues_count}")

def print_detailed_report(results: List[Dict], ticker: str = None):
    """Print detailed report for specific ticker or top/bottom performers."""
    valid_results = [r for r in results if 'error' not in r]
    
    if ticker:
        # Show specific ticker
        ticker_results = [r for r in valid_results if r['ticker'].upper() == ticker.upper()]
        if not ticker_results:
            print(f"❌ Ticker {ticker} not found in results")
            return
        result = ticker_results[0]
        print_ticker_details(result)
    else:
        # Show top and bottom performers
        sorted_results = sorted(valid_results, key=lambda x: x['data_quality_score'], reverse=True)
        
        print(f"\n🏆 TOP 5 QUALITY FILES:")
        for i, result in enumerate(sorted_results[:5], 1):
            print(f"  {i}. {result['ticker']}: {result['data_quality_score']:.1f}/100")
        
        print(f"\n⚠️  BOTTOM 5 QUALITY FILES:")
        for i, result in enumerate(sorted_results[-5:], 1):
            print(f"  {i}. {result['ticker']}: {result['data_quality_score']:.1f}/100")

def print_ticker_details(result: Dict):
    """Print detailed analysis for a specific ticker."""
    print(f"\n" + "=" * 60)
    print(f"📊 DETAILED ANALYSIS: {result['ticker']}")
    print("=" * 60)
    
    print(f"📁 File: {result['file_path']}")
    print(f"💾 Size: {result['file_size_mb']:.2f} MB")
    print(f"📏 Rows: {result['total_rows']:,}")
    print(f"🎯 Quality Score: {result['data_quality_score']:.1f}/100")
    
    print(f"\n📅 Date Range:")
    print(f"  • Start: {result['start_date'].strftime('%Y-%m-%d')}")
    print(f"  • End: {result['end_date'].strftime('%Y-%m-%d')}")
    print(f"  • Duration: {result['date_range_days']:,} days")
    print(f"  • Days since last update: {result['days_since_last_update']}")
    
    print(f"\n📋 Columns: {result['columns']}")
    
    if result['missing_columns']:
        print(f"❌ Missing columns: {result['missing_columns']}")
    
    if result['extra_columns']:
        print(f"➕ Extra columns: {result['extra_columns']}")
    
    if result['total_nulls'] > 0:
        print(f"\n🕳️  Null Values:")
        for col, count in result['null_counts'].items():
            if count > 0:
                print(f"  • {col}: {count:,} ({count/result['total_rows']*100:.1f}%)")
    
    if result['price_stats']:
        stats = result['price_stats']
        print(f"\n💰 Price Statistics:")
        print(f"  • Range: ${stats['min_price']:.2f} - ${stats['max_price']:.2f}")
        print(f"  • Mean: ${stats['mean_price']:.2f}")
        print(f"  • Median: ${stats['median_price']:.2f}")
        print(f"  • Std Dev: ${stats['std_price']:.2f}")
        if 'mean_daily_return' in stats:
            print(f"  • Avg Daily Return: {stats['mean_daily_return']*100:.3f}%")
            print(f"  • Daily Return Volatility: {stats['std_daily_return']*100:.3f}%")
    
    if result['volume_stats']:
        stats = result['volume_stats']
        print(f"\n📊 Volume Statistics:")
        print(f"  • Range: {stats['min_volume']:,} - {stats['max_volume']:,}")
        print(f"  • Mean: {stats['mean_volume']:,.0f}")
        print(f"  • Median: {stats['median_volume']:,.0f}")
        if stats['zero_volume_days'] > 0:
            print(f"  • Zero volume days: {stats['zero_volume_days']}")
    
    if result['consistency_issues']:
        print(f"\n⚠️  Consistency Issues:")
        for issue in result['consistency_issues']:
            print(f"  • {issue}")

def main():
    """Main function with command line interface."""
    parser = argparse.ArgumentParser(description="Review CSV output files from stock data downloader")
    parser.add_argument("directory", nargs="?", default="test_data", 
                       help="Directory containing CSV files (default: test_data)")
    parser.add_argument("--ticker", "-t", help="Show detailed analysis for specific ticker")
    parser.add_argument("--pattern", "-p", default="*.csv", help="File pattern to match (default: *.csv)")
    parser.add_argument("--export", "-e", help="Export results to CSV file")
    
    args = parser.parse_args()
    
    # Check if directory exists, if not try common directories
    if not Path(args.directory).exists():
        common_dirs = ["test_data", "stock_data", "etf_data", "data"]
        found_dir = None
        for dir_name in common_dirs:
            if Path(dir_name).exists():
                found_dir = dir_name
                break
        
        if found_dir:
            print(f"📁 Directory '{args.directory}' not found, using '{found_dir}' instead")
            args.directory = found_dir
        else:
            print(f"❌ No data directories found. Tried: {common_dirs}")
            return
    
    # Analyze files
    results = review_directory(args.directory, args.pattern)
    
    if not results:
        return
    
    # Print reports
    print_summary_report(results)
    print_detailed_report(results, args.ticker)
    
    # Export results if requested
    if args.export:
        export_results(results, args.export)

def export_results(results: List[Dict], filename: str):
    """Export analysis results to CSV."""
    try:
        # Flatten the results for CSV export
        export_data = []
        for r in results:
            if 'error' in r:
                export_data.append({
                    'ticker': r['ticker'],
                    'error': r['error'],
                    'quality_score': 0
                })
            else:
                row = {
                    'ticker': r['ticker'],
                    'quality_score': r['data_quality_score'],
                    'total_rows': r['total_rows'],
                    'file_size_mb': r['file_size_mb'],
                    'start_date': r['start_date'],
                    'end_date': r['end_date'],
                    'date_range_days': r['date_range_days'],
                    'has_recent_data': r['has_recent_data'],
                    'days_since_last_update': r['days_since_last_update'],
                    'total_nulls': r['total_nulls'],
                    'consistency_issues_count': len(r['consistency_issues'])
                }
                
                # Add price stats if available
                if r['price_stats']:
                    row.update({
                        'min_price': r['price_stats'].get('min_price'),
                        'max_price': r['price_stats'].get('max_price'),
                        'mean_price': r['price_stats'].get('mean_price'),
                        'mean_daily_return': r['price_stats'].get('mean_daily_return')
                    })
                
                export_data.append(row)
        
        df = pd.DataFrame(export_data)
        df.to_csv(filename, index=False)
        print(f"\n📄 Results exported to: {filename}")
        
    except Exception as e:
        print(f"❌ Error exporting results: {e}")

if __name__ == "__main__":
    main() 